{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando blibiotecas\n",
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "# !pip install numpy\n",
    "# !pip install xgboost\n",
    "# !pip install lightgbm\n",
    "# !pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81447,
     "status": "ok",
     "timestamp": 1761941823117,
     "user": {
      "displayName": "Caio Tomaz",
      "userId": "01025468359002248467"
     },
     "user_tz": 180
    },
    "id": "Tu5oPf990JKX",
    "outputId": "16c66339-ae04-411b-9148-a9fe847b4fc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignorando pasta 'backup' pois o nome não é um ano válido.\n",
      "DataFrame combinado criado com sucesso!\n",
      "  EMPRESA ORIGEM DESTINO   TARIFA  ASSENTOS  MES   ANO\n",
      "0     ABJ   SBSV    SIRI   650,00        17    1  2023\n",
      "1     ABJ   SBSV    SIRI   850,00        23    1  2023\n",
      "2     ABJ   SBSV    SIRI  1050,00         6    1  2023\n",
      "3     ABJ   SBSV    SIRI  1250,00         1    1  2023\n",
      "4     ABJ   SBSV    SNCL   450,00         1    1  2023\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define o caminho para a pasta que contém as pastas de ano\n",
    "base_folder_path = './dataset/'\n",
    "\n",
    "# Cria uma lista para armazenar os DataFrames de cada arquivo\n",
    "df_list = []\n",
    "\n",
    "# Lista todos os itens na pasta base\n",
    "all_items = os.listdir(base_folder_path)\n",
    "\n",
    "# Filtra apenas as pastas (que devem ser os anos)\n",
    "year_folders = [d for d in all_items if os.path.isdir(os.path.join(base_folder_path, d))]\n",
    "\n",
    "# Itera sobre as pastas de ano\n",
    "for year_folder in year_folders:\n",
    "    year_path = os.path.join(base_folder_path, year_folder)\n",
    "\n",
    "    # Extrai o ano do nome da pasta\n",
    "    try:\n",
    "        year_from_folder = int(year_folder)\n",
    "    except ValueError:\n",
    "        print(f\"Ignorando pasta '{year_folder}' pois o nome não é um ano válido.\")\n",
    "        continue # Skip to the next folder if the name is not a valid year\n",
    "\n",
    "    # Lista todos os arquivos na pasta do ano\n",
    "    year_files = os.listdir(year_path)\n",
    "\n",
    "    # Filtra apenas os arquivos CSV dentro da pasta do ano\n",
    "    csv_files = [f for f in year_files if f.endswith('.CSV')]\n",
    "\n",
    "    # Itera sobre os arquivos CSV dentro da pasta do ano\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(year_path, csv_file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep=';')  # Specify semicolon delimiter\n",
    "\n",
    "            # Use the year from the folder\n",
    "            df['ANO'] = year_from_folder\n",
    "\n",
    "            # Convert 'MES' to string before removing comma and converting to numeric\n",
    "            df['MES'] = df['MES'].astype(str).str.replace(',', '', regex=False) # Remove comma\n",
    "            df['MES'] = pd.to_numeric(df['MES'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "            # Create a simple date format (YYYY-MM) - assuming day is not important or always 1\n",
    "            df['DATA'] = pd.to_datetime(df['ANO'].astype(str) + '-' + df['MES'].astype(str), format='%Y-%m', errors='coerce')\n",
    "\n",
    "            # Extract year and month into new columns\n",
    "            df['ANO'] = df['DATA'].dt.year\n",
    "            df['MES'] = df['DATA'].dt.month\n",
    "\n",
    "            # Drop the original date columns\n",
    "            df = df.drop(columns=['nr_ano_referencia', 'nr_mes_referencia', 'DATA'], errors='ignore')\n",
    "\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo {csv_file} na pasta {year_folder}: {e}\")\n",
    "\n",
    "# Concatena todos os DataFrames na lista em um único DataFrame\n",
    "if df_list:\n",
    "    main_df = pd.concat(df_list, ignore_index=True)\n",
    "    print(\"DataFrame combinado criado com sucesso!\")\n",
    "\n",
    "    # Reorder columns\n",
    "    cols = main_df.columns.tolist()\n",
    "    cols.remove('MES')\n",
    "    cols.remove('ANO')\n",
    "    cols.insert(cols.index('ASSENTOS') + 1, 'MES')\n",
    "    cols.insert(cols.index('ASSENTOS') + 2, 'ANO')\n",
    "    main_df = main_df[cols]\n",
    "\n",
    "    print(main_df.head())\n",
    "else:\n",
    "    print(\"Nenhum arquivo CSV encontrado ou lido nas pastas de ano.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "executionInfo": {
     "elapsed": 8900,
     "status": "ok",
     "timestamp": 1761941832029,
     "user": {
      "displayName": "Caio Tomaz",
      "userId": "01025468359002248467"
     },
     "user_tz": 180
    },
    "id": "cFnYP4zFB0xP",
    "outputId": "b8c4eac6-aaed-418f-901e-864417ddec6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'EMPRESA' column after replacement:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['ATA - AEROTÁXI ABAETÉ LTDA.',\n",
       "       'AZUL LINHAS AÉREAS BRASILEIRAS S/A',\n",
       "       'GOL LINHAS AÉREAS S.A. (EX- VRG LINHAS AÉREAS S.A.)', 'PTB',\n",
       "       'TAM LINHAS AÉREAS S.A.', 'APUÍ TÁXI AÉREO S/A'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       EMPRESA ORIGEM DESTINO   TARIFA  ASSENTOS  MES   ANO\n",
      "0  ATA - AEROTÁXI ABAETÉ LTDA.   SBSV    SIRI   650,00        17    1  2023\n",
      "1  ATA - AEROTÁXI ABAETÉ LTDA.   SBSV    SIRI   850,00        23    1  2023\n",
      "2  ATA - AEROTÁXI ABAETÉ LTDA.   SBSV    SIRI  1050,00         6    1  2023\n",
      "3  ATA - AEROTÁXI ABAETÉ LTDA.   SBSV    SIRI  1250,00         1    1  2023\n",
      "4  ATA - AEROTÁXI ABAETÉ LTDA.   SBSV    SNCL   450,00         1    1  2023\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary with the mapping of abbreviations to full names\n",
    "empresa_mapping = {\n",
    "    'ABJ': 'ATA - AEROTÁXI ABAETÉ LTDA.',\n",
    "    'AZU': 'AZUL LINHAS AÉREAS BRASILEIRAS S/A',\n",
    "    'GLO': 'GOL LINHAS AÉREAS S.A. (EX- VRG LINHAS AÉREAS S.A.)',\n",
    "    'TAM': 'TAM LINHAS AÉREAS S.A.',\n",
    "    'CQB': 'APUÍ TÁXI AÉREO S/A'\n",
    "}\n",
    "\n",
    "# Replace the abbreviations in the 'EMPRESA' column\n",
    "main_df['EMPRESA'] = main_df['EMPRESA'].replace(empresa_mapping)\n",
    "\n",
    "# Display the updated unique values in the 'EMPRESA' column to verify\n",
    "print(\"Unique values in 'EMPRESA' column after replacement:\")\n",
    "display(main_df['EMPRESA'].unique())\n",
    "\n",
    "print(main_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1761941832246,
     "user": {
      "displayName": "Caio Tomaz",
      "userId": "01025468359002248467"
     },
     "user_tz": 180
    },
    "id": "3Tpg5vTTDetC",
    "outputId": "3722101d-3eb3-41c2-835d-d6b85563f774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame created with 'CÓDIGO OACI', 'MUNICÍPIO ATENDIDO', and 'UF' columns:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CÓDIGO OACI</th>\n",
       "      <th>MUNICÍPIO ATENDIDO</th>\n",
       "      <th>UF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SBAA</td>\n",
       "      <td>CONCEIÇÃO DO ARAGUAIA</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SBAE</td>\n",
       "      <td>BAURU</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SBAQ</td>\n",
       "      <td>ARARAQUARA</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SBAR</td>\n",
       "      <td>ARACAJU</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SBAT</td>\n",
       "      <td>ALTA FLORESTA</td>\n",
       "      <td>MT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CÓDIGO OACI     MUNICÍPIO ATENDIDO  UF\n",
       "0        SBAA  CONCEIÇÃO DO ARAGUAIA  PA\n",
       "1        SBAE                  BAURU  SP\n",
       "2        SBAQ             ARARAQUARA  SP\n",
       "3        SBAR                ARACAJU  SE\n",
       "4        SBAT          ALTA FLORESTA  MT"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_file_path = './dataset/cadastro-de-aerodromos-civis-publicos.csv'\n",
    "\n",
    "try:\n",
    "    airport_df = pd.read_csv(csv_file_path, sep=';')\n",
    "\n",
    "    # Include the 'UF' column\n",
    "    airport_df = airport_df[['CÓDIGO OACI', 'MUNICÍPIO ATENDIDO', 'UF']]\n",
    "\n",
    "    print(\"New DataFrame created with 'CÓDIGO OACI', 'MUNICÍPIO ATENDIDO', and 'UF' columns:\")\n",
    "    display(airport_df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_file_path}\")\n",
    "except KeyError:\n",
    "    print(\"Error: 'CÓDIGO OACI', 'MUNICÍPIO ATENDIDO', or 'UF' columns not found in the CSV.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "executionInfo": {
     "elapsed": 65071,
     "status": "ok",
     "timestamp": 1761941897940,
     "user": {
      "displayName": "Caio Tomaz",
      "userId": "01025468359002248467"
     },
     "user_tz": 180
    },
    "id": "9wswJo9p8kN1",
    "outputId": "8c38a4e2-a0fe-421a-f84b-44827aef7cbc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Passo 1: Limpar e padronizar os dados\n",
    "main_df_clean = main_df.copy()\n",
    "airport_df_clean = airport_df.copy()\n",
    "\n",
    "# Limpar espaços e converter para maiúsculas\n",
    "main_df_clean['ORIGEM'] = main_df_clean['ORIGEM'].str.strip().str.upper()\n",
    "main_df_clean['DESTINO'] = main_df_clean['DESTINO'].str.strip().str.upper()\n",
    "airport_df_clean['CÓDIGO OACI'] = airport_df_clean['CÓDIGO OACI'].str.strip().str.upper()\n",
    "\n",
    "if 'CÓDIGO IATA' in airport_df_clean.columns:\n",
    "    airport_df_clean['CÓDIGO IATA'] = airport_df_clean['CÓDIGO IATA'].str.strip().str.upper()\n",
    "\n",
    "# Passo 2: Merge para ORIGEM (que já estava funcionando)\n",
    "merged_origin_df = pd.merge(\n",
    "    main_df_clean,\n",
    "    airport_df_clean,\n",
    "    left_on='ORIGEM',\n",
    "    right_on='CÓDIGO OACI',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Renomear e reorganizar ORIGEM\n",
    "merged_origin_df = merged_origin_df.rename(columns={'MUNICÍPIO ATENDIDO': 'MUNICIPIO_ORIGEM'})\n",
    "merged_origin_df = merged_origin_df.drop(columns=['CÓDIGO OACI', 'UF', 'ORIGEM'], errors='ignore')\n",
    "merged_origin_df = merged_origin_df.rename(columns={'MUNICIPIO_ORIGEM': 'ORIGEM'})\n",
    "\n",
    "# Reordenar colunas\n",
    "cols = merged_origin_df.columns.tolist()\n",
    "cols.remove('ORIGEM')\n",
    "cols.insert(cols.index('EMPRESA') + 1, 'ORIGEM')\n",
    "merged_origin_df = merged_origin_df[cols]\n",
    "\n",
    "# Passo 3: Merge para DESTINO com verificação\n",
    "print(\"Verificando merge para DESTINO...\")\n",
    "\n",
    "# Tentar merge com OACI\n",
    "merged_final_df = pd.merge(\n",
    "    merged_origin_df,\n",
    "    airport_df_clean,\n",
    "    left_on='DESTINO',\n",
    "    right_on='CÓDIGO OACI',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Verificar resultado\n",
    "matches_destino = merged_final_df['MUNICÍPIO ATENDIDO'].notna().sum()\n",
    "print(f\"Matches encontrados para DESTINO: {matches_destino}/{len(merged_final_df)}\")\n",
    "\n",
    "# Se não encontrou matches, mostrar exemplos problemáticos\n",
    "if matches_destino == 0:\n",
    "    print(\"\\nCódigos problemáticos em DESTINO:\")\n",
    "    problematic_codes = merged_origin_df['DESTINO'].unique()[:10]\n",
    "    for code in problematic_codes:\n",
    "        print(f\"  '{code}' -> Existe em airport_df? {code in airport_df_clean['CÓDIGO OACI'].values}\")\n",
    "\n",
    "# Continuar com o processamento independente do resultado\n",
    "merged_final_df = merged_final_df.rename(columns={'MUNICÍPIO ATENDIDO': 'MUNICIPIO_DESTINO'})\n",
    "merged_final_df = merged_final_df.drop(columns=['CÓDIGO OACI', 'UF', 'DESTINO'], errors='ignore')\n",
    "merged_final_df = merged_final_df.rename(columns={'MUNICIPIO_DESTINO': 'DESTINO'})\n",
    "\n",
    "# Reordenar colunas\n",
    "cols = merged_final_df.columns.tolist()\n",
    "cols.remove('DESTINO')\n",
    "cols.insert(cols.index('ORIGEM') + 1, 'DESTINO')\n",
    "merged_final_df = merged_final_df[cols]\n",
    "\n",
    "# Remover linhas onde ORIGEM ou DESTINO são nulos ou NaN\n",
    "rows_before = len(merged_final_df)\n",
    "merged_final_df = merged_final_df.dropna(subset=['ORIGEM', 'DESTINO'])\n",
    "rows_after = len(merged_final_df)\n",
    "rows_removed = rows_before - rows_after\n",
    "print(f\"\\nLinhas removidas com ORIGEM ou DESTINO nulos: {rows_removed} ({(rows_removed/rows_before)*100:.2f}% do total)\")\n",
    "\n",
    "# Display the head of the final dataframe to verify\n",
    "print(\"\\nDataFrame after merging destination airport municipality and removing null values:\")\n",
    "display(merged_final_df.head())\n",
    "print(f\"\\nShape do DataFrame final: {merged_final_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121991,
     "status": "ok",
     "timestamp": 1761942019952,
     "user": {
      "displayName": "Caio Tomaz",
      "userId": "01025468359002248467"
     },
     "user_tz": 180
    },
    "id": "k9sgNQgtB9LA",
    "outputId": "d1e2806d-09c5-4e35-9225-cb7878f4c1f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame exportado com sucesso para ./validacao.csv\n"
     ]
    }
   ],
   "source": [
    "# Define o caminho e nome do arquivo para exportar\n",
    "output_path = './validacao.csv'\n",
    "\n",
    "# Exporta o DataFrame para CSV\n",
    "try:\n",
    "    merged_final_df.to_csv(output_path, index=False)\n",
    "    print(f\"DataFrame exportado com sucesso para {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao exportar o DataFrame: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42413,
     "status": "ok",
     "timestamp": 1761942062998,
     "user": {
      "displayName": "Caio Tomaz",
      "userId": "01025468359002248467"
     },
     "user_tz": 180
    },
    "id": "RfonZmZTCJ_S",
    "outputId": "b33b90bf-6114-4b48-f5a3-a0aa99e3ffab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de valores únicos em cada coluna:\n",
      "EMPRESA: 5 valores únicos\n",
      "ORIGEM: 166 valores únicos\n",
      "DESTINO: 165 valores únicos\n",
      "\n",
      "Shape do DataFrame X após encoding: (17765412, 6)\n",
      "\n",
      "Uso de memória otimizado para as features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Otimizar tipos de dados para reduzir uso de memória\n",
    "merged_final_df['TARIFA'] = merged_final_df['TARIFA'].astype(str).str.replace(',', '.').astype('float32')\n",
    "merged_final_df['MES'] = merged_final_df['MES'].astype('int8')\n",
    "merged_final_df['ANO'] = merged_final_df['ANO'].astype('int16')\n",
    "merged_final_df['ASSENTOS'] = merged_final_df['ASSENTOS'].astype('int32')\n",
    "\n",
    "# Verificar cardinalidade das colunas categóricas\n",
    "print(\"Número de valores únicos em cada coluna:\")\n",
    "for col in ['EMPRESA', 'ORIGEM', 'DESTINO']:\n",
    "    print(f\"{col}: {merged_final_df[col].nunique()} valores únicos\")\n",
    "\n",
    "# Variável alvo\n",
    "y = merged_final_df['ASSENTOS'].values\n",
    "\n",
    "# Criar cópia das features numéricas com tipos otimizados\n",
    "X = merged_final_df[['TARIFA', 'MES', 'ANO']].copy()\n",
    "\n",
    "# Aplicar Label Encoding para cada coluna categórica\n",
    "encoders = {}\n",
    "for col in ['EMPRESA', 'ORIGEM', 'DESTINO']:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(merged_final_df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "    X[col] = X[col].astype('int32')  # Otimizar tipo após encoding\n",
    "\n",
    "print(\"\\nShape do DataFrame X após encoding:\", X.shape)\n",
    "print(\"\\nUso de memória otimizado para as features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7517,
     "status": "ok",
     "timestamp": 1761942070528,
     "user": {
      "displayName": "Caio Tomaz",
      "userId": "01025468359002248467"
     },
     "user_tz": 180
    },
    "id": "8a6ZzehCCMJK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados divididos e validação cruzada configurada.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Dividir dados com estratificação para melhor distribuição\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configurar validação cruzada\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return r2, rmse\n",
    "\n",
    "print(\"Dados divididos e validação cruzada configurada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6865,
     "status": "ok",
     "timestamp": 1761942077411,
     "user": {
      "displayName": "Caio Tomaz",
      "userId": "01025468359002248467"
     },
     "user_tz": 180
    },
    "id": "nntG2cyHCNuC",
    "outputId": "f4517573-80f5-4eb9-d5d6-f2a7574b8b63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressão Linear - R²: 0.0125, RMSE: 18.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "print(f\"Regressão Linear - R²: {r2_lr:.4f}, RMSE: {rmse_lr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 129066,
     "status": "ok",
     "timestamp": 1761942206495,
     "user": {
      "displayName": "Caio Tomaz",
      "userId": "01025468359002248467"
     },
     "user_tz": 180
    },
    "id": "9fd741a6",
    "outputId": "a38294f9-c3c0-4db0-81de-c9f0c088039b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - R²: 0.0477, RMSE: 18.52\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Configurar XGBoost com parâmetros otimizados\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'early_stopping_rounds': 20\n",
    "}\n",
    "\n",
    "# Treinar XGBoost\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "print(f\"XGBoost - R²: {r2_xgb:.4f}, RMSE: {rmse_xgb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 138253,
     "status": "ok",
     "timestamp": 1761942344726,
     "user": {
      "displayName": "Caio Tomaz",
      "userId": "01025468359002248467"
     },
     "user_tz": 180
    },
    "id": "1QhmcUsh_9TO",
    "outputId": "1dbf3ff9-72ab-410d-93f9-ceb245f129c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM - R²: 0.0491, RMSE: 18.51\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Configurar LightGBM com parâmetros otimizados\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'early_stopping_round': 20,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Treinar LightGBM\n",
    "lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "lgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "y_pred_lgbm = lgb_model.predict(X_test)\n",
    "r2_lgbm = r2_score(y_test, y_pred_lgbm)\n",
    "rmse_lgbm = np.sqrt(mean_squared_error(y_test, y_pred_lgbm))\n",
    "print(f\"LightGBM - R²: {r2_lgbm:.4f}, RMSE: {rmse_lgbm:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOSk5cFn0ZTN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting - R²: 0.0379, RMSE: 18.62\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Treinar Gradient Boosting\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "print(f\"Gradient Boosting - R²: {r2_gb:.4f}, RMSE: {rmse_gb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTzRi3Xg0lgR"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Treinar AdaBoost\u001b[39;00m\n\u001b[32m      6\u001b[39m ada_model = AdaBoostRegressor(n_estimators=\u001b[32m100\u001b[39m, learning_rate=\u001b[32m0.1\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mada_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m y_pred_ada = ada_model.predict(X_test)\n\u001b[32m     10\u001b[39m r2_ada = r2_score(y_test, y_pred_ada)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Caio Tomaz\\Desktop\\flytics\\venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Caio Tomaz\\Desktop\\flytics\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:167\u001b[39m, in \u001b[36mBaseWeightBoosting.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    164\u001b[39m sample_weight[zero_weight_mask] = \u001b[32m0.0\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# Boosting step\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m sample_weight, estimator_weight, estimator_error = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_boost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# Early termination\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Caio Tomaz\\Desktop\\flytics\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:1068\u001b[39m, in \u001b[36mAdaBoostRegressor._boost\u001b[39m\u001b[34m(self, iboost, X, y, sample_weight, random_state)\u001b[39m\n\u001b[32m   1066\u001b[39m X_ = _safe_indexing(X, bootstrap_idx)\n\u001b[32m   1067\u001b[39m y_ = _safe_indexing(y, bootstrap_idx)\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1069\u001b[39m y_predict = estimator.predict(X)\n\u001b[32m   1071\u001b[39m error_vect = np.abs(y_predict - y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Caio Tomaz\\Desktop\\flytics\\venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Caio Tomaz\\Desktop\\flytics\\venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[39m, in \u001b[36mDecisionTreeRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m   1374\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1376\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[32m   1377\u001b[39m \n\u001b[32m   1378\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1401\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Caio Tomaz\\Desktop\\flytics\\venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Treinar AdaBoost\n",
    "ada_model = AdaBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "ada_model.fit(X_train, y_train)\n",
    "y_pred_ada = ada_model.predict(X_test)\n",
    "\n",
    "r2_ada = r2_score(y_test, y_pred_ada)\n",
    "rmse_ada = np.sqrt(mean_squared_error(y_test, y_pred_ada))\n",
    "print(f\"AdaBoost - R²: {r2_ada:.4f}, RMSE: {rmse_ada:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
