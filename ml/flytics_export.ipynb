{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instalando blibiotecas\n",
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "# !pip install numpy\n",
    "# !pip install xgboost\n",
    "# !pip install lightgbm\n",
    "# !pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81447,
     "status": "ok",
     "timestamp": 1761941823117,
     "user": {
      "displayName": "Caio Tomaz",
      "userId": "01025468359002248467"
     },
     "user_tz": 180
    },
    "id": "Tu5oPf990JKX",
    "outputId": "16c66339-ae04-411b-9148-a9fe847b4fc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROCESSANDO DADOS DE VOOS (modo otimizado) ===\n",
      "Lendo (chunks) ‚Üí ./dataset/2023/202301.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2023/202304.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2023/202310.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2023/202303.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2023/202308.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2023/202312.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2023/202309.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2023/202305.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2023/202302.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2023/202311.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2023/202306.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2023/202307.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2024/202407.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2024/202401.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2024/202402.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2024/202405.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2024/202411.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2024/202403.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2024/202404.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2024/202412.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2024/202409.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2024/202410.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2024/202406.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2024/202408.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2025/202502.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2025/202505.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2025/202501.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2025/202503.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2025/202504.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2025/202507.CSV\n",
      "Lendo (chunks) ‚Üí ./dataset/2025/202506.CSV\n",
      "Ignorando pasta 'backup' pois o nome n√£o √© ano.\n",
      "Leitura dos voos conclu√≠da. Criando main_df...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9360/2882746123.py:69: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  main_df = pd.read_csv(TEMP_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_df criado com sucesso! Shape: (17873941, 7)\n",
      "Mapeamento de empresas conclu√≠do!\n",
      "\n",
      "=== PROCESSANDO DADOS DE AEROPORTOS ===\n",
      "airport_df criado! (504, 3)\n",
      "\n",
      "=== INICIANDO MERGE FINAL EM CHUNKS ===\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9360/2882746123.py:129: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(TEMP_FILE, chunksize=CHUNK_SIZE):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9360/2882746123.py:129: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(TEMP_FILE, chunksize=CHUNK_SIZE):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9360/2882746123.py:129: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(TEMP_FILE, chunksize=CHUNK_SIZE):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9360/2882746123.py:129: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(TEMP_FILE, chunksize=CHUNK_SIZE):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9360/2882746123.py:129: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(TEMP_FILE, chunksize=CHUNK_SIZE):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9360/2882746123.py:129: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(TEMP_FILE, chunksize=CHUNK_SIZE):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 200000 linhas...\n",
      "Processando merge chunk ‚Üí 73941 linhas...\n",
      "=== MERGE EM CHUNKS CONCLU√çDO ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9360/2882746123.py:171: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  final_df = pd.read_csv(MERGED_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESULTADO FINAL ===\n",
      "Shape final: (17765412, 9)\n",
      "    ANO  MES EMPRESA ORIGEM DESTINO  TARIFA  ASSENTOS ORIGEM.1       DESTINO.1\n",
      "0  2023    1     AZU   SBAC    SBAR  851,90         2  ARACATI         ARACAJU\n",
      "1  2023    1     AZU   SBAC    SBFI  734,90         2  ARACATI   FOZ DO IGUA√áU\n",
      "2  2023    1     AZU   SBAC    SBGL  832,90         1  ARACATI  RIO DE JANEIRO\n",
      "3  2023    1     AZU   SBAC    SBGR  630,90         4  ARACATI       GUARULHOS\n",
      "4  2023    1     AZU   SBAC    SBGR  928,90         2  ARACATI       GUARULHOS\n"
     ]
    }
   ],
   "source": [
    "# ===== SESS√ÉO 1 + 2 + 3 (VERS√ÉO OTIMIZADA PARA GRANDES VOLUMES) =====\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"=== PROCESSANDO DADOS DE VOOS (modo otimizado) ===\")\n",
    "base_folder_path = './dataset/'\n",
    "\n",
    "# Arquivo tempor√°rio (streaming)\n",
    "TEMP_FILE = \"temp_voos.csv\"\n",
    "if os.path.exists(TEMP_FILE):\n",
    "    os.remove(TEMP_FILE)\n",
    "\n",
    "df_list = []\n",
    "first_write = True\n",
    "\n",
    "all_items = os.listdir(base_folder_path)\n",
    "year_folders = [d for d in all_items if os.path.isdir(os.path.join(base_folder_path, d))]\n",
    "\n",
    "# ===== PARTE 1 ‚Äî LEITURA EM CHUNKS (SEM ESTOURAR MEM√ìRIA) =====\n",
    "for year_folder in year_folders:\n",
    "    year_path = os.path.join(base_folder_path, year_folder)\n",
    "\n",
    "    try:\n",
    "        year_from_folder = int(year_folder)\n",
    "    except ValueError:\n",
    "        print(f\"Ignorando pasta '{year_folder}' pois o nome n√£o √© ano.\")\n",
    "        continue\n",
    "\n",
    "    year_files = os.listdir(year_path)\n",
    "    # Ignorar arquivos .~lock\n",
    "    csv_files = [\n",
    "        f for f in year_files\n",
    "        if f.endswith('.CSV') and not f.startswith('.~lock')\n",
    "    ]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(year_path, csv_file)\n",
    "        print(f\"Lendo (chunks) ‚Üí {file_path}\")\n",
    "\n",
    "        try:\n",
    "            chunk_iter = pd.read_csv(file_path, sep=';', chunksize=100_000)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao abrir {csv_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for chunk in chunk_iter:\n",
    "            # TRATAMENTO DO CHUNK\n",
    "            chunk['ANO'] = year_from_folder\n",
    "            chunk['MES'] = chunk['MES'].astype(str).str.replace(',', '', regex=False)\n",
    "            chunk['MES'] = pd.to_numeric(chunk['MES'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "            chunk['DATA'] = pd.to_datetime(\n",
    "                chunk['ANO'].astype(str) + '-' + chunk['MES'].astype(str),\n",
    "                format='%Y-%m', errors='coerce'\n",
    "            )\n",
    "            chunk['ANO'] = chunk['DATA'].dt.year\n",
    "            chunk['MES'] = chunk['DATA'].dt.month\n",
    "\n",
    "            chunk = chunk.drop(columns=['nr_ano_referencia', 'nr_mes_referencia', 'DATA'], errors='ignore')\n",
    "\n",
    "            # Salvar em streaming no arquivo tempor√°rio\n",
    "            chunk.to_csv(TEMP_FILE, mode='a', index=False, header=first_write)\n",
    "            first_write = False\n",
    "\n",
    "print(\"Leitura dos voos conclu√≠da. Criando main_df...\")\n",
    "\n",
    "# Carregar arquivo j√° unido\n",
    "try:\n",
    "    main_df = pd.read_csv(TEMP_FILE)\n",
    "    print(f\"main_df criado com sucesso! Shape: {main_df.shape}\")\n",
    "except Exception as e:\n",
    "    print(\"Erro ao criar main_df:\", e)\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "# Reordenar colunas\n",
    "if not main_df.empty:\n",
    "    cols = main_df.columns.tolist()\n",
    "    if 'MES' in cols and 'ANO' in cols and 'ASSENTOS' in cols:\n",
    "        cols.remove('MES')\n",
    "        cols.remove('ANO')\n",
    "        cols.insert(cols.index('ASSENTOS') + 1, 'MES')\n",
    "        cols.insert(cols.index('ASSENTOS') + 2, 'ANO')\n",
    "        main_df = main_df[cols]\n",
    "\n",
    "    # Mapear empresas\n",
    "    empresa_mapping = {\n",
    "        'ABJ': 'ATA - AEROT√ÅXI ABAET√â LTDA.',\n",
    "        'AZU': 'AZUL LINHAS A√âREAS BRASILEIRAS S/A',\n",
    "        'GLO': 'GOL LINHAS A√âREAS S.A. (EX- VRG LINHAS A√âREAS S.A.)',\n",
    "        'TAM': 'TAM LINHAS A√âREAS S.A.',\n",
    "        'CQB': 'APU√ç T√ÅXI A√âREO S/A'\n",
    "    }\n",
    "    if 'EMPRESA' in main_df.columns:\n",
    "        main_df['EMPRESA'] = main_df['EMPRESA'].replace(empresa_mapping)\n",
    "        print(\"Mapeamento de empresas conclu√≠do!\")\n",
    "\n",
    "# ===== PARTE 2 ‚Äî AEROPORTOS =====\n",
    "print(\"\\n=== PROCESSANDO DADOS DE AEROPORTOS ===\")\n",
    "csv_file_path = './dataset/cadastro-de-aerodromos-civis-publicos.csv'\n",
    "\n",
    "try:\n",
    "    airport_df = pd.read_csv(csv_file_path, sep=';')\n",
    "    airport_df = airport_df[['C√ìDIGO OACI', 'MUNIC√çPIO ATENDIDO', 'UF']]\n",
    "    print(\"airport_df criado!\", airport_df.shape)\n",
    "except Exception as e:\n",
    "    print(\"Erro ao carregar airport_df:\", e)\n",
    "    airport_df = pd.DataFrame()\n",
    "\n",
    "# ===== PARTE 3 ‚Äî MERGE FINAL EM CHUNKS (SEM ESTOURAR A MEM√ìRIA) =====\n",
    "\n",
    "print(\"\\n=== INICIANDO MERGE FINAL EM CHUNKS ===\")\n",
    "\n",
    "MERGED_FILE = \"temp_merged.csv\"\n",
    "if os.path.exists(MERGED_FILE):\n",
    "    os.remove(MERGED_FILE)\n",
    "\n",
    "if main_df.empty or airport_df.empty:\n",
    "    print(\"ERRO: Um dos DataFrames est√° vazio.\")\n",
    "else:\n",
    "    # Padronizar airport_df apenas 1 vez\n",
    "    airport_df_clean = airport_df.copy()\n",
    "    airport_df_clean['C√ìDIGO OACI'] = airport_df_clean['C√ìDIGO OACI'].astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Ler main_df em chunks direto do arquivo temp_voos.csv\n",
    "    CHUNK_SIZE = 200_000\n",
    "\n",
    "    first_write_merge = True\n",
    "\n",
    "    for chunk in pd.read_csv(TEMP_FILE, chunksize=CHUNK_SIZE):\n",
    "\n",
    "        print(f\"Processando merge chunk ‚Üí {len(chunk)} linhas...\")\n",
    "\n",
    "        # Padronizar\n",
    "        chunk['ORIGEM'] = chunk['ORIGEM'].astype(str).str.strip().str.upper()\n",
    "        chunk['DESTINO'] = chunk['DESTINO'].astype(str).str.strip().str.upper()\n",
    "\n",
    "        # üîπ MERGE ORIGEM\n",
    "        chunk = chunk.merge(\n",
    "            airport_df_clean,\n",
    "            left_on='ORIGEM',\n",
    "            right_on='C√ìDIGO OACI',\n",
    "            how='left'\n",
    "        ).rename(columns={'MUNIC√çPIO ATENDIDO': 'MUNICIPIO_ORIGEM'}) \\\n",
    "         .drop(columns=['C√ìDIGO OACI', 'UF'], errors='ignore')\n",
    "\n",
    "        # üîπ MERGE DESTINO\n",
    "        chunk = chunk.merge(\n",
    "            airport_df_clean,\n",
    "            left_on='DESTINO',\n",
    "            right_on='C√ìDIGO OACI',\n",
    "            how='left'\n",
    "        ).rename(columns={'MUNIC√çPIO ATENDIDO': 'MUNICIPIO_DESTINO'}) \\\n",
    "         .drop(columns=['C√ìDIGO OACI', 'UF'], errors='ignore')\n",
    "\n",
    "        # Reorganizar colunas\n",
    "        if 'MUNICIPIO_ORIGEM' in chunk.columns:\n",
    "            chunk = chunk.rename(columns={'MUNICIPIO_ORIGEM': 'ORIGEM'})\n",
    "        if 'MUNICIPIO_DESTINO' in chunk.columns:\n",
    "            chunk = chunk.rename(columns={'MUNICIPIO_DESTINO': 'DESTINO'})\n",
    "\n",
    "        # Remover linhas inv√°lidas\n",
    "        chunk = chunk.dropna(subset=['ORIGEM', 'DESTINO'])\n",
    "\n",
    "        # üî• Salvar incrementalmente\n",
    "        chunk.to_csv(MERGED_FILE, index=False, mode='a', header=first_write_merge)\n",
    "        first_write_merge = False\n",
    "\n",
    "    print(\"=== MERGE EM CHUNKS CONCLU√çDO ===\")\n",
    "\n",
    "# Carregar DataFrame final consolidado\n",
    "final_df = pd.read_csv(MERGED_FILE)\n",
    "\n",
    "print(\"\\n=== RESULTADO FINAL ===\")\n",
    "print(f\"Shape final: {final_df.shape}\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42413,
     "status": "ok",
     "timestamp": 1761942062998,
     "user": {
      "displayName": "Caio Tomaz",
      "userId": "01025468359002248467"
     },
     "user_tz": 180
    },
    "id": "RfonZmZTCJ_S",
    "outputId": "b33b90bf-6114-4b48-f5a3-a0aa99e3ffab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de valores √∫nicos em cada coluna:\n",
      "EMPRESA: 5 valores √∫nicos\n",
      "ORIGEM: 170 valores √∫nicos\n",
      "DESTINO: 169 valores √∫nicos\n",
      "\n",
      "Shape do DataFrame X ap√≥s encoding: (17765412, 5)\n",
      "\n",
      "Uso de mem√≥ria otimizado para as features.\n"
     ]
    }
   ],
   "source": [
    "# Prepara√ß√£o dos dados para modelagem com otimiza√ß√£o de mem√≥ria\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "df = final_df  # renomear para facilitar\n",
    "\n",
    "# Otimizar tipos de dados para reduzir uso de mem√≥ria\n",
    "df['TARIFA'] = df['TARIFA'].astype(str).str.replace(',', '.').astype('float32')\n",
    "df['MES'] = df['MES'].astype('int8')\n",
    "df['ANO'] = df['ANO'].astype('int16')\n",
    "df['ASSENTOS'] = df['ASSENTOS'].astype('int32')\n",
    "\n",
    "# Verificar cardinalidade das colunas categ√≥ricas\n",
    "print(\"N√∫mero de valores √∫nicos em cada coluna:\")\n",
    "for col in ['EMPRESA', 'ORIGEM', 'DESTINO']:\n",
    "    print(f\"{col}: {df[col].nunique()} valores √∫nicos\")\n",
    "\n",
    "# Vari√°vel alvo\n",
    "y = df['TARIFA']\n",
    "X = df[['EMPRESA', 'ORIGEM', 'DESTINO', 'MES', 'ANO']].copy()\n",
    "\n",
    "# Aplicar Label Encoding para colunas categ√≥ricas\n",
    "encoders = {}\n",
    "for col in ['EMPRESA', 'ORIGEM', 'DESTINO']:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "    X[col] = X[col].astype('int32')  # reduzir mem√≥ria\n",
    "\n",
    "print(\"\\nShape do DataFrame X ap√≥s encoding:\", X.shape)\n",
    "print(\"\\nUso de mem√≥ria otimizado para as features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7517,
     "status": "ok",
     "timestamp": 1761942070528,
     "user": {
      "displayName": "Caio Tomaz",
      "userId": "01025468359002248467"
     },
     "user_tz": 180
    },
    "id": "8a6ZzehCCMJK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados divididos e valida√ß√£o cruzada configurada.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Dividir dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# K-Fold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return r2, rmse\n",
    "\n",
    "print(\"Dados divididos e valida√ß√£o cruzada configurada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 138253,
     "status": "ok",
     "timestamp": 1761942344726,
     "user": {
      "displayName": "Caio Tomaz",
      "userId": "01025468359002248467"
     },
     "user_tz": 180
    },
    "id": "1QhmcUsh_9TO",
    "outputId": "1dbf3ff9-72ab-410d-93f9-ceb245f129c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM - R¬≤: 0.0912, RMSE: 590.08\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Par√¢metros otimizados\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'n_estimators': 200,\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'early_stopping_rounds': 20,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Treinar LightGBM\n",
    "lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "# Avaliar\n",
    "y_pred_lgbm = lgb_model.predict(X_test)\n",
    "r2_lgbm = r2_score(y_test, y_pred_lgbm)\n",
    "rmse_lgbm = np.sqrt(mean_squared_error(y_test, y_pred_lgbm))\n",
    "\n",
    "print(f\"LightGBM - R¬≤: {r2_lgbm:.4f}, RMSE: {rmse_lgbm:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportando o Modelo e Encoders\n",
    "Vamos salvar o modelo LightGBM treinado e os encoders necess√°rios para uso posterior na API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo e dados auxiliares exportados com sucesso!\n",
      "Modelo salvo em: ./model_export2/lightgbm_model.pkl\n",
      "Encoders salvos em: ./model_export2/encoders.pkl\n",
      "Informa√ß√µes categ√≥ricas salvas em: ./model_export2/categorical_info.json\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "import json\n",
    "\n",
    "model_dir = './model_export2'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Salvar modelo\n",
    "model_path = os.path.join(model_dir, 'lightgbm_model.pkl')\n",
    "joblib.dump(lgb_model, model_path)\n",
    "\n",
    "# Salvar encoders\n",
    "encoders_path = os.path.join(model_dir, 'encoders.pkl')\n",
    "joblib.dump(encoders, encoders_path)\n",
    "\n",
    "# Salvar informa√ß√µes categ√≥ricas\n",
    "categorical_info = {\n",
    "    'empresas': df['EMPRESA'].unique().tolist(),\n",
    "    'origens': df['ORIGEM'].unique().tolist(),\n",
    "    'destinos': df['DESTINO'].unique().tolist()\n",
    "}\n",
    "\n",
    "info_path = os.path.join(model_dir, 'categorical_info.json')\n",
    "with open(info_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(categorical_info, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"\\nModelo e dados auxiliares exportados com sucesso!\")\n",
    "print(f\"Modelo salvo em: {model_path}\")\n",
    "print(f\"Encoders salvos em: {encoders_path}\")\n",
    "print(f\"Informa√ß√µes categ√≥ricas salvas em: {info_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# GERAR HIST√ìRICO DE PRE√áOS POR ROTA\n",
    "# ============================================\n",
    "\n",
    "# Verificar colunas esperadas\n",
    "required_cols = [\"ORIGEM\", \"DESTINO\", \"TARIFA\"]\n",
    "missing = [c for c in required_cols if c not in final_df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Colunas ausentes no final_df: {missing}\")\n",
    "\n",
    "print(\"Gerando hist√≥rico de pre√ßos para todas rotas...\")\n",
    "\n",
    "# Agrupar por rota\n",
    "grouped = final_df.groupby([\"ORIGEM\", \"DESTINO\"])[\"TARIFA\"]\n",
    "\n",
    "historico = []\n",
    "\n",
    "for (orig, dest), series in grouped:\n",
    "    prices = series.dropna().astype(float).tolist()\n",
    "    if len(prices) == 0:\n",
    "        continue\n",
    "    \n",
    "    historico.append({\n",
    "        \"origin\": orig,\n",
    "        \"destination\": dest,\n",
    "        \"min_price\": float(min(prices)),\n",
    "        \"max_price\": float(max(prices)),\n",
    "        \"prices\": prices  # caso queira s√≥ min/max, pode remover\n",
    "    })\n",
    "\n",
    "# Caminho do backend\n",
    "output_path = Path(\".model_export2/\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Salvar JSON\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(historico, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Hist√≥rico salvo com sucesso em: {output_path}\")\n",
    "print(f\"Total de rotas salvas: {len(historico)}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
